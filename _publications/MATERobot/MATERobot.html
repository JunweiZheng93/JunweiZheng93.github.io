<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
  body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }

  h1 {
    font-weight: 300;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
  }

  a:link,
  a:visited {
    color: #1B9783;
    text-decoration: none;
  }

  a:hover {
    color: #345daa;
    text-decoration:underline;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big {
    /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
      0px 0px 1px 1px rgba(0, 0, 0, 0.35),
      /* The top layer shadow */
      5px 5px 0 0px #fff,
      /* The second layer */
      5px 5px 1px 1px rgba(0, 0, 0, 0.35),
      /* The second layer shadow */
      10px 10px 0 0px #fff,
      /* The third layer */
      10px 10px 1px 1px rgba(0, 0, 0, 0.35),
      /* The third layer shadow */
      15px 15px 0 0px #fff,
      /* The fourth layer */
      15px 15px 1px 1px rgba(0, 0, 0, 0.35),
      /* The fourth layer shadow */
      20px 20px 0 0px #fff,
      /* The fifth layer */
      20px 20px 1px 1px rgba(0, 0, 0, 0.35),
      /* The fifth layer shadow */
      25px 25px 0 0px #fff,
      /* The fifth layer */
      25px 25px 1px 1px rgba(0, 0, 0, 0.35);
    /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper {
    /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
      0px 0px 1px 1px rgba(0, 0, 0, 0.35),
      /* The top layer shadow */
      5px 5px 0 0px #fff,
      /* The second layer */
      5px 5px 1px 1px rgba(0, 0, 0, 0.35),
      /* The second layer shadow */
      10px 10px 0 0px #fff,
      /* The third layer */
      10px 10px 1px 1px rgba(0, 0, 0, 0.35);
    /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
    top: 50%;
    transform: translateY(-50%);
  }

  hr {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>
<script type="text/javascript" src="resources/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic'
  rel='stylesheet' type='text/css'>

  <!-- ----------------------------------------- Meta ----------------------------------------------- -->
<head>
  <link rel="icon" type="image/png" href="/img/icon.png">
  <title>MATERobot</title>
  <meta property='og:title' content='MATERobot' />
  <meta property="og:description"
    content="Zheng et al. MATERobot" />
  <meta property='og:url' content='https://github.com/JunweiZheng93/MATERobot' />
  <meta property='og:video' content='' />
  <meta property='og:image' content='' />
  <meta property="og:type" content="video">
</head>

<body>
  <br>
  <!-- ----------------------------------------- Title ----------------------------------------------- -->
  <center><span style="font-size:46px;font-weight:bold;">MATERobot: Material Recognition in Wearable Robotics for People with Visual Impairments</span></center>
  <center><span style="font-size:28px;">Preprint: arXiv</span></center>

  <table align=center width=900px>
    <tr>
      <td align=center width=150px>
        <center><span style="font-size:20px"><a href="https://junweizheng93.github.io/" target="_blank">Junwei Zheng<sup>1,</sup>*</a></span></center>
      </td>
      <td align=center width=150px>
        <center><span style="font-size:20px"><a href="https://jamycheung.github.io/" target="_blank">Jiaming Zhang<sup>1,</sup>*</a></span></center>
      </td>
      <td align=center width=150px>
        <center><span style="font-size:20px"><a href="https://yangkailun.com/" target="_blank">Kailun Yang<sup>2,†</sup></a></span></center>
      </td>
      <td align=center width=150px>
        <center><span style="font-size:20px"><a href="https://scholar.google.com/citations?user=pA9c0YsAAAAJ&hl=en" target="_blank">Kunyu Peng<sup>1</sup></a></span></center>
      </td>
      <td align=center width=200px>
        <center><span style="font-size:20px"><a href="https://cvhci.anthropomatik.kit.edu/people_596.php" target="_blank">Rainer Stiefelhagen<sup>1</sup></a></span></center>
      </td>
    <tr />
  </table>
  <table align=center width=500px>
    <tr>
      <td align=center width=250px>
        <center><span style="font-size:18px"><sup>1</sup>Karlsruhe Institute of Technology</span></center>
      </td>
      <td align=center width=150px>
        <center><span style="font-size:18px"><sup>2</sup>Hunan University</span></center>
      </td>
    <tr />
  </table>
  <tr /></tr>
  <table align=center width=250px>
    <tr>
      <td align=center width=100px>
        <center><span style="font-size:24px"><a href="https://arxiv.org/pdf/2302.14595.pdf">[PDF]</a></span></center>
      </td>
      <td align=center width=100px>
        <center><span style="font-size:24px"><a href='https://github.com/JunweiZheng93/MATERobot'>[Code]</a></span>
        </center>
      </td>
    <tr />
  </table><br />

  <hr>
  <br>

  <!-- ----------------------------------------- Abstract ----------------------------------------------- -->

  <center id="abstract">
    <h1>Abstract</h1>
  </center>
  <p align="justify">
    Wearable robotics can improve the lives of People with Visual Impairments (PVI) by providing additional sensory information. Blind people typically recognize objects through haptic perception. However, knowing materials before touching is under-explored in the field of assistive technology. To fill this gap, in this work, a wearable robotic system, MATERobot, is established for PVI to recognize materials before hand. Specially, the human-centric system can perform pixel-wise semantic segmentation of objects and materials. Considering both general object segmentation and material segmentation, an efficient MateViT architecture with Learnable Importance Sampling (LIS) and Multi-gate Mixture-of-Experts (MMoE) is proposed to wearable robots to achieve complementary gains from different target domains. Our methods achieve respective <b>40.2%</b> and <b>51.1%</b> of mIoU on COCOStuff and DMS datasets, surpassing previous method with +<b>5.7%</b> and +<b>7.0%</b> gains. Moreover, on the field test with participants, our wearable system obtains a score of <b>28</b> in NASA-Task Load Index, indicating low cognitive demands and ease of use. Our MATERobot demonstrates the feasibility of recognizing material properties through visual cues, and offers a promising step towards improving the functionality of wearable robots for PVI. 
  </p>

  <br>
  <hr>
  <br>

  <!-- ----------------------------------------- Body ----------------------------------------------- -->

  <center id="robot">
    <h1>MATERobot</h1>
  </center>
  <table align=center width=1000px>
    <tr>
      <td width=400px>
        <center><img src="images/MATERobot.png" height="300px"></img>
          <p>Fig. 1: MATERobot, (a) wearable robotics, can assist (b) <u>material</u> semantic segmentation (<i>e.g.</i>, <u><span style="color:#BD33A4;">snow</span></u>, <u><span style="color:gray;">ceramic</span></u>) and general <i>object</i> semantic segmentation (<i>e.g.</i>, <i><span style="color:#f013ee;">sidewalk</span></i>, <i><span style="color:blue;">cup</span></i>)</p>
        </center>
      </td>
    </tr>
  </table>
  <p align="justify">
     In this work, object and material recognition in wearable robotic systems are presented, <i>i.e.</i>, MATERobot. As shown in Fig. 1a, such a human-friendly system consists of a pair of smart glasses with a RGB-D camera, a pair of bone-conducting earphones, and a portable image processor inside a small waist bag. In Fig. 1b, the MATERobot can recognize not only materials (<i>e.g.</i>, <u><span style="color:#BD33A4;">snow</span></u>), but also general object categories (<i>e.g.</i>, <i><span style="color:#f013ee;">sidewalk</span></i>). These two predictions from different fields can form a feedback of object categories with material properties (<i>e.g.</i>, “<u><span style="color:#BD33A4;">snowy</span></u> <i><span style="color:#f013ee;">sidewalk</span></i>”), yielding more comprehensive information to assist PVI in their daily life.
  </p>

<!--  <br>-->
<!--  <br>-->
<!--  <hr>-->

<!--  <center id="demoVideo">-->
<!--    <h1>Demo Video</h1>-->
<!--  </center>-->
<!--  <table align=center width=600px>-->
<!--    <tr>-->
<!--      <td align=center width=600px>-->
<!--        <iframe width="800" height="450" src="" frameborder="0" allowfullscreen></iframe>-->
<!--      </td>-->
<!--    </tr>-->
<!--  </table>-->

<!--  <br>-->
<!--  <hr>-->

  <br>
  <hr>
  <br>

  <center id="model">
    <h1>MateViT Model</h1>
  </center>
  <table align=center width=1000px>
    <tr>
      <td width=1000px>
        <center><img src="images/model_architecture.png" height="400px"></img><br>
          <p>Fig. 2: Model architecture of MateViT with Learnable Importance Sampling (LIS) strategy to reduce the computational complexity, and a Multi-gate Mixture-of-Experts (MMoE) layer to perform dual-task segmentation (<i>i.e.</i>, #1-Object and #2-Material segmentation). MHSA and FFN stand for Multi-Head Self-Attention and Feed-Forward Network. The upsampling layer is a standard Transformer decoder block. Decoder heads are the segmentation heads of Segmenter.</p>
        </center>
      </td>
    </tr>
  </table>
  <p align="justify">
    MateViT is our proposed model deployed on the MATERobot wearable system. For the sake of achieving high performance and efficiency, we propose MateViT, which has ViT with Learnable Importance Sampling (LIS) as the backbone, followed by an upsampling layer, a Multi-gate Mixture-of- Experts (MMoE) layer and two decoder heads. The overall architecture is shown in Fig. 2.
  </p>
  <center id="lis">
    <h2>Learnable Importance Sampling</h2>
  </center>
  <p align="justify">
   Since softmax is introduced in attention map calculation, the summation of each value in rows is equal to 1, illustrated in Fig. 2. However, the result does not always equal 1 when summing up all values in columns, indicating the importance of each token from an image. Based on this observation, we first calculate the summation in columns and then average the importance vectors among all heads. Top k values are selected and tokens are downsampled according to the indices of these k values after Add & Norm, which stand for a residual link and layer normalization.
  </p>
  <center id="mmoe">
    <h2>Multi-gate Mixture-of-Experts</h2>
  </center>
  <p align="justify">
    During the training, for every token in one image, only one same gate is activated to produce the selection vector. According to the indices of the top m values, m experts are selected and the token is only fed into the m experts. The output of the MMoE layer is a weighted sum of the top m values in the selection vector and their corresponding outcomes from the m experts. A task-relevant decoder head is then applied to transform all output tokens from MMoE into a prediction mask. During the inference, every token from one image is fed into all gates synchronously. The resulting weighted sums from selected experts are then decoded by the corresponding decoder heads shown in Fig. 2.
  </p>
  
  <br>
  <hr>
  <br>

  <center id="experiments">
    <h1>Experiments</h1>
  </center>
  <center id="single">
    <h2>Single-task Learning</h2>
  </center>
  <table align=center width=800px>
    <tr>
      <td width=400px>
        <center><img src="images/coco.png" height="320px"></img><br>
          <p>Table 1: Results of single-task learning on COCOStuff-10K test set. GLOPs are measured in resolution 512x512.</p>
        </center>
      </td>
      <td width=400px>
        <center><img src="images/dms.png" height="320"></img><br>
          <p>Table 2: Results of single-task learning on DMS dataset. Values are for the val/test set, respectively. GLOPs are measured in resolution 512x512.</p>
        </center>
      </td>
    </tr>
  </table>
  <p align="justify">
  To conduct experiments for single-task learning, we remove the MMoE layer shown in Fig.2 but keep the LIS strategy. Since our purpose is to make plain-ViT more efficient and feasible in real-world applications, for a fair comparison, we mainly study on plain-ViT-based models. Table 1 summarizes the quantitative comparison with other state-of-the-art methods on COCOStuff-10K test set, while Table 2 show the results on DMS val/test set. It can be easily observed that our MateViT with ViT-Tiny backbone achieves the lowest computation complexity on both datasets (<b>4.35</b> GFLOPs on COCOStuff-10K, <b>4.30</b> GFLOPs on DMS) and keeps competitive performance among the listed counterparts. Moreover, our ViT-Small variant surpasses the others with <b>70.2%</b> pixel accuracy and <b>38.9%</b> mIoU on COCOStuff-10K. Same improvements can also be observed on DMS dataset. These results verify the superiority and efficiency of MateViT with learnable importance sampling for general object and material segmentation.
  </p>
  <table align=center width=1000px>
    <tr>
      <td width=700px>
        <center><img src="images/category_iou.png" height="370px"></img><br>
          <p>Fig. 3: Per-class IoU (%) of all material categories. The blue bar shows the category IoU (%) of the baseline DMS, while the orange shows the performance gains (%) of our ViT-Small variant.</p>
        </center>
      </td>
    </tr>
  </table>
  <p align="justify">
  Fig. 3 presents the per-class IoU of all material categories. It is worth noting that our ViT-Small variant achieves performance gains in all 46 categories, especially the ones that are relevant for assisting PVI, e.g., fire (gain +<b>20.2%</b>), snow (gain +<b>21.3%</b>), plastic (gain +<b>22.6%</b>), and ceramic (gain +<b>25.5%</b>). The aforementioned results prove that our model with learnable importance sampling is capable of achieving high performance and efficiency, not only on the object segmentation but also on the material segmentation task. The high IoU scores ensure the effectiveness of our proposed system to assist PVI to recognize materials.
  </p>
  <center id="multi">
    <h2>Multi-task Learning</h2>
  </center>
  <table align=center width=1000px>
    <tr>
      <td width=500px>
        <center><img src="images/coco_dms.png" height="200px"></img><br>
          <p>Table 3: Results (mIoU) of multi-task learning on COCOStuff-10k and DMS test sets. GFLOPs are measured in resolution 512×512.</p>
        </center>
      </td>
    </tr>
  </table>
  <p align="justify">
  The mIoU comparison to other multi-task learning method, Tran4Trans, is illustrated in Table 3. Our ViT-Tiny version achieves <b>35.0%</b> fewer GLOPs compared with Trans4Trans MiT-B0.  In general, the ViT-based models do have a larger number of parameters than the baselines. Compared with the number of parameters (MParams) affecting the size of the model, the computational complexity more critically determines the efficiency of the model running on the mobile platform, therefore, affects the fluency of the entire system. According to the single-task results in Table 1 and Table 2, it is worth noting that there are sufficient performance improvements when adding the MMoE layer to our multi-task learning model. The mIoU of ViT-Tiny model is raised from <b>31.6%</b> (Table 1) to <b>32.7%</b> (Table 3) on object segmentation and from <b>44.1%</b> (Table 2) to <b>45.1%</b> (Table 3) on material segmentation. The performance gains are consistent in the ViT-Small variant, from <b>38.9%</b> to <b>40.2%</b> and from <b>50.1%</b> to <b>51.1%</b> on object and material segmentation, respectively. The reason for the gains is two-fold: (1) the MMoE layer enlarges the model capacity; (2) when applying MMoE to both object segmentation and material segmentation, the former prevents the latter from overfitting and vice versa.
  </p>
  <center id="ablation">
    <h2>Ablation Study</h2>
  </center>
  <table align=center width=1000px>
    <tr>
      <td width=700px>
        <center><img src="images/ablation.png" height="200px"></img><br>
          <p>Table 4: Results (mIoU) of multi-task learning on COCOStuff-10k and DMS test sets. GFLOPs are measured in resolution 512×512.</p>
        </center>
      </td>
    </tr>
  </table>
  <p align="justify">
  To fully understand the proposed methods, an ablation study is conducted regarding the different modules of our model. The quantitative results are shown in Table 4. Our baseline model is the same as Segmenter ViT-Tiny. Learned from Table 4, replacing ViT-Tiny with ViT-Small as the backbone boosts the performance from <b>43.2%</b> to <b>49.2%</b> on mIoU in material segmentation task. After applying LIS, the mIoU continuously increases from <b>49.2%</b> to <b>50.1%</b>. We then add the MoE layer to our model, leading to an even higher mIoU of <b>50.7%</b>. These performance gains also occur on other metrics and other tasks. It can be observed that our MMoE model achieves the best performances on all metrics in both object segmentation and material segmentation tasks compared to the baseline model, <i>i.e.</i>, <b>6.5%</b> and <b>3.7%</b> boosts in pixel accuracy, <b>8.9%</b> and <b>7.9%</b> boosts in mIoU. Through the extensive pre-study experiments, the effectiveness of the proposed method can be comprehensively proved.
  </p>
  <center id="qualitative_results">
    <h2>Qualitative Results</h2>
  </center>
  <table align=center width=1000px>
    <tr>
      <td width=800px>
        <center><img src="images/seg_results.png" height="370px"></img><br>
          <p>Fig. 4: Visualization of both object and material segmentation. From left to right in each group: RGB input, object segmentation, material segmentation.</p>
        </center>
      </td>
    </tr>
  </table>
  <p align="justify">
    Four groups of scenarios related to the daily life of PVI are shown in Fig. 4. The group in the lower left corner shows a scenario where blind people walk in a park. According to the predictions in the second and third images, blind people using our system know additionally there is an asphaltic road ahead. With the additional material information, they can traverse through the road more safely. In the bottom-right case, the entrance is not recognized if only object recognition is performed, however, the material segmentation result can provide supplementary information to find the doors, which can further improve the mobility accessibility. As a result, these dense information can help PVI better understand their surroundings, and to support them to make correct interactions with the environment.
  </p>

  <br>
  <hr>
  <br>

  <center id="user_study">
    <h1>User Study</h1>
  </center>
  <center id="organization">
    <h2>Organization</h2>
  </center>
  <table align=center width=1000px>
    <tr>
      <td width=800px>
        <center><img src="images/participants.png" height="300px"></img><br>
          <p>Fig. 5: Incidences of participants on the user study.</p>
        </center>
      </td>
    </tr>
  </table>
  <p align="justify">
  To verify the usability of the system, we further organize a user study with six blindfolded participants on real-world testing scenarios. For the user study, we select seven categories of materials that are common in daily life, <i>i.e.</i>, fabric, foliage, glass, metal, paper, plastic, and wood. To conduct a comparison between without and with using our system, there are two rounds of material recognition, <i>i.e.</i>, contact and contactless round. The contact round is to recognize by touch, while the contactless round is to recognize by using our system. Note that, to conduct a fair comparison, the order of the objects in two rounds is randomized. Six blindfolded participants (three males and three females) are invited to conduct both testing rounds in the user study, as presented in Fig. 5. In the contact round, the participants are required to name the material after touching an object. The reaction time from touching the object until naming the material is recorded by the organizer. In the contactless round, the participants are required to name the material after hearing the feedback from the bone-conduction earphones of the wearable system. The reaction time from starting our system until naming the material is also recorded for the sake of comparison. Apart from the recorded reaction time, answer correctness is another metric of this user study. After two rounds of material recognition, all participants join an anonymous questionnaire session. The questionnaires regarding NASA-Task Load Index (NASA-TLX) and System Usability Scale (SUS) are filled out by all participants. Besides, there is space for participants to write down their open comments.
  </p>
  <center id="user_study_results">
    <h2>Results</h2>
  </center>
  <table align=center width=1000px>
    <tr>
      <td width=500px>
        <center><img src="images/reaction_time.png" height="150px"></img><br>
          <p>Table 5: Average reaction time and answer correctness in two rounds of material recognition.</p>
        </center>
      </td>
    </tr>
  </table>
  <p align="justify">
  We utilize average reaction time and answer correctness to evaluate our system performance. Table 5 presents results on the aforementioned metrics among all participants. While the answer correctness is (<i>100%</i>) in both rounds, the average reaction time in the contactless round is obviously shorter than the contact round (response −<i>0.86</i>s). Without any haptic perception, our wearable system can perform a faster recognition than the contact-based perception, which indicates our MATERobot is feasible and reliable to provide fast assistance in recognizing material properties via visual cues, and it is able to offer a promising step towards improving the perception accessibility for PVI. 
  </p>
  <table align=center width=1000px>
    <tr>
      <td width=500px>
        <center><img src="images/tlx.png" height="350px"></img><br>
          <p>Fig. 6: Average NASA-TLX score of every factor among all participants. Values range from 0 to 100, lower is better.</p>
        </center>
      </td>
    </tr>
  </table>
  <p align="justify">
  To learn about the cognitive load of our wearable system, NASA-TLX is a simple and effective method for cognitive load measurement. We first calculate the average score of every factor among all participants. There are six factors, which are Mental Demand, Physical Demand, Temporal Demand, Performance, Effort, and Frustration, as shown in Fig. 6. We then average the scores of all six factors, resulting in a final NASA-TLX value of <b>28</b>. According to "<i>How high is high? A meta-analysis of nasa-tlx global workload scores</i>", this value illustrates the workload caused by our system is in the 20th percentile of global workload scores from <b>6.21</b> to <b>88.50</b> among <b>1173</b> observations, which can assist users without too much burden. From Fig. 6, we notice that the effort value is relatively smaller than the rest five factors, meaning that our system is user-friendly. Apart from NASA-TLX, we also verify the usability of our system with SUS. Our system scores <b>77</b> out of <b>100</b>, which is a relatively high score. According to "<i>An empirical evaluation of the system usability scale</i>" of Bangor <i>et al.</i>, who analyzed <b>2324</b> surveys from <b>206</b> studies, “the best quarter of studies range from <b>78.51</b> to <b>93.93</b>”. Therefore, we reach that our system is useful for recognizing not only general objects but also their material properties.
  </p>

  <br>
  <hr>
  <br>

  <!-- ----------------------------------------- Conclusion ----------------------------------------------- -->

  <center id="conclusion">
    <h1>Conclusion</h1>
  </center>
  <p align="justify">
  In this work, we look into semantic material understanding for helping visually impaired people via a wearable robotic system MATERobot. We put forward MateViT, which uni- fies general object and material segmentation via a multi- gate mixture-of-experts architecture, whose efficiency is en- hanced via learnable importance sampling to make plain- vision-transformer models suitable for mobile applications. The proposed model is ported to our established assistive MATERobot system designed for supporting people with visual impairments. Extensive experiments on DMS and COCOStuff-10K datasets and a user study demonstrate the effectiveness and usefulness of our recognition system.
  </p>

  <br>
  <hr>
  <br>

  <!-- ----------------------------------------- Citation ----------------------------------------------- -->

  <center id="citation">
    <h1>Citation</h1>
  </center>
  <table align=center width=800px>
    <tr>
      <td width=800px align=left>
        <p style="text-align:left;"><br />
          <span style="font-size:15pt"><i>If you find our work useful in your research, please cite:</i></span>
        </p>          
        <pre xml:space="preserve">
@article{zheng2023materobot,
  title={MATERobot: Material Recognition in Wearable Robotics for People with Visual Impairments},
  author={Zheng, Junwei and Zhang, Jiaming and Yang, Kailun and Peng, Kunyu and Stiefelhagen, Rainer},
  journal={arXiv preprint arXiv:2302.14595},
  year={2023}
}
        </pre>
      </td>
    </tr>
  </table>

  <!-- ----------------------------------------- Footer ----------------------------------------------- -->

  <script xml:space="preserve" language="JavaScript">
    hideallbibs();
  </script>
  <footer class="site-footer">
    <center>
        <p class="powered-by">&copy; 2023 <a href='https://junweizheng93.github.io/'>Junwei Zheng</a>
        </p>
      </center>
</footer>
</body>

</html>