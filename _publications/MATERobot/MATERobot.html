<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
  body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }

  h1 {
    font-weight: 300;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
  }

  a:link,
  a:visited {
    color: #1B9783;
    text-decoration: none;
  }

  a:hover {
    color: #345daa;
    text-decoration:underline;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big {
    /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
      0px 0px 1px 1px rgba(0, 0, 0, 0.35),
      /* The top layer shadow */
      5px 5px 0 0px #fff,
      /* The second layer */
      5px 5px 1px 1px rgba(0, 0, 0, 0.35),
      /* The second layer shadow */
      10px 10px 0 0px #fff,
      /* The third layer */
      10px 10px 1px 1px rgba(0, 0, 0, 0.35),
      /* The third layer shadow */
      15px 15px 0 0px #fff,
      /* The fourth layer */
      15px 15px 1px 1px rgba(0, 0, 0, 0.35),
      /* The fourth layer shadow */
      20px 20px 0 0px #fff,
      /* The fifth layer */
      20px 20px 1px 1px rgba(0, 0, 0, 0.35),
      /* The fifth layer shadow */
      25px 25px 0 0px #fff,
      /* The fifth layer */
      25px 25px 1px 1px rgba(0, 0, 0, 0.35);
    /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper {
    /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
      0px 0px 1px 1px rgba(0, 0, 0, 0.35),
      /* The top layer shadow */
      5px 5px 0 0px #fff,
      /* The second layer */
      5px 5px 1px 1px rgba(0, 0, 0, 0.35),
      /* The second layer shadow */
      10px 10px 0 0px #fff,
      /* The third layer */
      10px 10px 1px 1px rgba(0, 0, 0, 0.35);
    /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
    top: 50%;
    transform: translateY(-50%);
  }

  hr {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>
<script type="text/javascript" src="resources/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic'
  rel='stylesheet' type='text/css'>

  <!-- ----------------------------------------- Meta ----------------------------------------------- -->
<head>
  <link rel="icon" type="image/png" href="/img/icon.png">
  <title>MATERobot</title>
  <meta property='og:title' content='MATERobot' />
  <meta property="og:description"
    content="Zheng et al. MATERobot" />
  <meta property='og:url' content='https://github.com/JunweiZheng93/MATERobot' />
  <meta property='og:video' content='' />
  <meta property='og:image' content='' />
  <meta property="og:type" content="video">
</head>

<body>
  <br>
  <!-- ----------------------------------------- Title ----------------------------------------------- -->
  <center><span style="font-size:46px;font-weight:bold;">MATERobot: Material Recognition in Wearable Robotics for People with Visual Impairments</span></center>
  <center><span style="font-size:28px;">Preprint: arXiv</span></center>

  <table align=center width=900px>
    <tr>
      <td align=center width=150px>
        <center><span style="font-size:20px"><a href="https://junweizheng93.github.io/" target="_blank">Junwei Zheng<sup>1,</sup>*</a></span></center>
      </td>
      <td align=center width=150px>
        <center><span style="font-size:20px"><a href="https://jamycheung.github.io/" target="_blank">Jiaming Zhang<sup>1,</sup>*</a></span></center>
      </td>
      <td align=center width=150px>
        <center><span style="font-size:20px"><a href="https://yangkailun.com/" target="_blank">Kailun Yang<sup>2,†</sup></a></span></center>
      </td>
      <td align=center width=150px>
        <center><span style="font-size:20px"><a href="https://scholar.google.com/citations?user=pA9c0YsAAAAJ&hl=en" target="_blank">Kunyu Peng<sup>1</sup></a></span></center>
      </td>
      <td align=center width=200px>
        <center><span style="font-size:20px"><a href="https://cvhci.anthropomatik.kit.edu/people_596.php" target="_blank">Rainer Stiefelhagen<sup>1</sup></a></span></center>
      </td>
    <tr />
  </table>
  <table align=center width=500px>
    <tr>
      <td align=center width=250px>
        <center><span style="font-size:18px"><sup>1</sup>Karlsruhe Institute of Technology</span></center>
      </td>
      <td align=center width=150px>
        <center><span style="font-size:18px"><sup>2</sup>Hunan University</span></center>
      </td>
    <tr />
  </table>
  </table>
  <table align=center width=500px>
    <tr>
      <td align=center width=250px>
        <center><span style="font-size:18px">*Joint First Authors</span></center>
      </td>
      <td align=center width=250px>
        <center><span style="font-size:18px">†Corresponding Author</span></center>
      </td>
    <tr />
  </table>
  <tr /></tr>
  <table align=center width=250px>
    <tr>
      <td align=center width=100px>
        <center><span style="font-size:24px"><a href="https://arxiv.org/pdf/2302.14595.pdf">[PDF]</a></span></center>
      </td>
      <td align=center width=100px>
        <center><span style="font-size:24px"><a href='https://github.com/JunweiZheng93/MATERobot'>[Code]</a></span>
        </center>
      </td>
    <tr />
  </table><br />

  <hr>
  <br>

  <!-- ----------------------------------------- Abstract ----------------------------------------------- -->

  <center id="abstract">
    <h1>Abstract</h1>
  </center>
  <p align="justify">
    Wearable robotics can improve the lives of People with Visual Impairments (PVI) by providing additional sensory information. Blind people typically recognize objects through haptic perception. However, knowing materials before touching is under-explored in the field of assistive technology. To fill this gap, in this work, a wearable robotic system, MATERobot, is established for PVI to recognize materials before hand. Specially, the human-centric system can perform pixel-wise semantic segmentation of objects and materials. Considering both general object segmentation and material segmentation, an efficient MateViT architecture with Learnable Importance Sampling (LIS) and Multi-gate Mixture-of-Experts (MMoE) is proposed to wearable robots to achieve complementary gains from different target domains. Our methods achieve respective <b>40.2%</b> and <b>51.1%</b> of mIoU on COCOStuff and DMS datasets, surpassing previous method with +<b>5.7%</b> and +<b>7.0%</b> gains. Moreover, on the field test with participants, our wearable system obtains a score of <b>28</b> in NASA-Task Load Index, indicating low cognitive demands and ease of use. Our MATERobot demonstrates the feasibility of recognizing material properties through visual cues, and offers a promising step towards improving the functionality of wearable robots for PVI. 
  </p>

  <br>
  <hr>
  <br>

  <!-- ----------------------------------------- Body ----------------------------------------------- -->

  <center id="robot">
    <h1>MATERobot</h1>
  </center>
  <table align=center width=1000px>
    <tr>
      <td width=400px>
        <center><img src="images/MATERobot.png" height="300px"></img>
          <p>Fig. 1: MATERobot, (a) wearable robotics, can assist (b) <u>material</u> semantic segmentation (<i>e.g.</i>, <u><span style="color:#BD33A4;">snow</span></u>, <u><span style="color:gray;">ceramic</span></u>) and general <i>object</i> semantic segmentation (<i>e.g.</i>, <i><span style="color:#f013ee;">sidewalk</span></i>, <i><span style="color:blue;">cup</span></i>)</p>
        </center>
      </td>
    </tr>
  </table>

  <br>
  <br>
  <hr>

<!--  <center id="demoVideo">-->
<!--    <h1>Demo Video</h1>-->
<!--  </center>-->
<!--  <table align=center width=600px>-->
<!--    <tr>-->
<!--      <td align=center width=600px>-->
<!--        <iframe width="800" height="450" src="" frameborder="0" allowfullscreen></iframe>-->
<!--      </td>-->
<!--    </tr>-->
<!--  </table>-->

<!--  <br>-->
<!--  <br>-->
<!--  <hr>-->


  <center id="model">
    <h1>MateViT Model</h1>
  </center>
  <table align=center width=1000px>
    <tr>
      <td width=1000px>
        <center><img src="images/model_architecture.png" height="400px"></img><br>
          <p>Fig. 2: Model architecture of MateViT with Learnable Importance Sampling (LIS) strategy to reduce the computational complexity, and a Multi-gate Mixture-of-Experts (MMoE) layer to perform dual-task segmentation (<i>i.e.</i>, #1-Object and #2-Material segmentation). MHSA and FFN stand for Multi-Head Self-Attention and Feed-Forward Network. The upsampling layer is a standard Transformer decoder block. Decoder heads are the segmentation heads of Segmenter.</p>
        </center>
      </td>
    </tr>
  </table>
  <p align="justify">
    MateViT is our proposed model deployed on the MATERobot wearable system. For the sake of achieving high performance and efficiency, we propose MateViT, which has ViT with Learnable Importance Sampling (LIS) as the backbone, followed by an upsampling layer, a Multi-gate Mixture-of- Experts (MMoE) layer and two decoder heads. The overall architecture is shown in Fig. 2.
  </p>
  <center id="lis">
    <h2>Learnable Importance Sampling</h2>
  </center>
  <p align="justify">
   Since softmax is introduced in attention map calculation, the summation of each value in rows is equal to 1, illustrated in Fig. 2. However, the result does not always equal 1 when summing up all values in columns, indicating the importance of each token from an image. Based on this observation, we first calculate the summation in columns and then average the importance vectors among all heads. Top k values are selected and tokens are downsampled according to the indices of these k values after Add & Norm, which stand for a residual link and layer normalization.
  </p>
  <center id="mmoe">
    <h2>Multi-gate Mixture-of-Experts</h2>
  </center>
  <p align="justify">
    During the training, for every token in one image, only one same gate is activated to produce the selection vector. According to the indices of the top m values, m experts are selected and the token is only fed into the m experts. The output of the MMoE layer is a weighted sum of the top m values in the selection vector and their corresponding outcomes from the m experts. A task-relevant decoder head is then applied to transform all output tokens from MMoE into a prediction mask. During the inference, every token from one image is fed into all gates synchronously. The resulting weighted sums from selected experts are then decoded by the corresponding decoder heads shown in Fig. 2.
  </p>
  
  <br>
  <hr>
  <br>

  <center id="results">
    <h1>Results</h1>
  </center>
  <table align=center width=1000px>
    <tr>
      <td width=700px>
        <center><img src="images/category_iou.png" height="370px"></img><br>
          <p>Fig. 3: Per-class IoU (%) of all material categories. The blue bar shows the category IoU (%) of the baseline DMS, while the orange shows the performance gains (%) of our ViT-Small variant.</p>
        </center>
      </td>
    </tr>
  </table>
  <table align=center width=1000px>
    <tr>
      <td width=800px>
        <center><img src="images/seg_results.png" height="370px"></img><br>
          <p>Fig. 4: Visualization of both object and material segmentation. From left to right in each group: RGB input, object segmentation, material segmentation.</p>
        </center>
      </td>
    </tr>
  </table>

  <br>
  <hr>
  <br>

  <!-- ----------------------------------------- Citation ----------------------------------------------- -->

  <center id="citation">
    <h1>Citation</h1>
  </center>
  <table align=center width=800px>
    <tr>
      <td width=800px align=left>
        <p style="text-align:left;"><br />
          <span style="font-size:15pt"><i>If you find our work useful in your research, please cite:</i></span>
        </p>          
        <pre xml:space="preserve">
@article{zheng2023materobot,
  title={MATERobot: Material Recognition in Wearable Robotics for People with Visual Impairments},
  author={Zheng, Junwei and Zhang, Jiaming and Yang, Kailun and Peng, Kunyu and Stiefelhagen, Rainer},
  journal={arXiv preprint arXiv:2302.14595},
  year={2023}
}
        </pre>
      </td>
    </tr>
  </table>

  <!-- ----------------------------------------- Footer ----------------------------------------------- -->

  <script xml:space="preserve" language="JavaScript">
    hideallbibs();
  </script>
  <footer class="site-footer">
    <center>
        <p class="powered-by">&copy; 2023 <a href='https://junweizheng93.github.io/'>Junwei Zheng</a>
        </p>
      </center>
</footer>
</body>

</html>